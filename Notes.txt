supervised learning
    we'll be having labels and based on that we predict the outcome -> regression and classification .

1.regression house price prediction-> take in multiple values and then we map it on a graph
    based on this graph we predict the outcome of the data 
2. classification -> it's a either yes or no question 
    if there are more than 2 categories we use classification based on categories based on parameters
    as we add more parameters the model goes complex
    therefore usage of multiple parameters is always useful

un-supervised learning
    the data is'nt labelled therefore we don't know what should be done, therefore we won't be having any parameters -> categorization
    1.in unsupervised we group the data which are similar and classfy them as group 1 and etc.. 

    data only comes with input x and we dont have output labels y, hence algorithim has to find structure in the data

    -> clustering
    -> dimensionality reduction
    

introduction to pandas 
    type        pandas name 
    vector      .series
    (1-dim)
    matrix      .dataframe

linear regression model is all about minimizing the cost error by using linear regression function and finding the lowest part of the graph 


06062023
Linear Regression model part 1:                                                                   

    what is linear regression?
    -> finding the best fit line fit for your data
    
    how to find the best fit line? -> y=mx+c [m? , c?, we have a input x and it predicts output y]
        we use F(x) = wX + b : where w,b are parameters/coefficients/weights
        where x = 'input' - 'feature'
        where y = 'output' - 'target' 
        m = number of training examples

        for each values are refrenced by x^i and y^i where i is the position of the object in the dataset

        1. find the cost function : 

                the distance from the line to the actual point J(w,b) = 1/2m(sum|x^i - x^^i|) : where m is the number of table entries
            therefore cost = mean squared error .

            using the above formula we get the value of the J[w] and J[b].

            once we plot it in 2d we get a parabola.

            if we also want to include both the values i.e w and b then its a type of a hammock in 3d .

            in order to increase the accuracy of the model we need to minimize the cost of the model hence we need something called


        2. Gradient descent: 
            w = w- a*diff(J[w,b])/diff(w) // where a is the learning rate decided by us 
            similarly 
            b = b- a*diff(J[w,b])/diff(b)

            in order to get the minimum function we need to run them simultaneously as w depends on b 
            therefore we need to decide the learning rate as optimally as 

            how do we run them simultaneously?
            -> we store the w and b in temporary value and then update them simultaneously
                temp_w =  w- a*diff(J[w,b])/diff(w) 
                temp_b =  b- a*diff(J[w,b])/diff(b)

                w = temp_w
                b = temp_b
            
            if there are multiple minimas then we use squared error cost to find any one minima 

        3. multiple parameters: 
            it would be catastrophic to just include only one feature and predict the outcome based on that 
            therefore we use multiple factors like number of bedrooms , how new the house is etc. therefore 
            instead of using multiple factors into the equation we use vectors 
            i.e -> F(x) = w.X + b 
            where w.X = w1x1+ w2x2+ w3x3+ w4x4+ ... wnxn.
            
            w and x can be represented as a matrix and can be multiplied consecutively to get the dot product 
            we can also use f = np.dot(w,x) //vectorised calculation
            therefore we get w and x for each home respetively 
                i.e w1,w2,w3,w4 etc. 
                we can either run a gradient descent on a single value or better consider them as a vector and run gradient descent on vectorised values 


        3.2 alternative for gradient descent 
            -> NOrmal Equation:
                only for linear regression with one variable 
                doesnt generalise to other learning algotithims 
                Normal method may be used in libraries but gradient descent is used widely cause it accounts in mutliple
                features and vector calculation can be done easily as well

        what are hyperparameters ? 
            Hyperparameters are parameters that are set before the training of a machine learning model begins.
            They control the learning process and the model parameters that result from it
            e.g learning rate

        practical tips for Linear Regression : 
            1.Feature Scaling -> if we know the w1 and w2 .. values we can prioritise the weights accordingly
                1.1 feature size and parameter size -> scaling up and down to our requirements
                    how do we scale it down or up ? 
                        1.mean normalization 
                            x1 = x1-U / max-min -> we'll get a value in the range of -1 to 1 [we do so to reduce the calculations]
                        2.z score normalization 
                            x1 = x1-U / variance of the data -> it varies on the data eg: -3 to 3
            
                1.2. make sure the learning rate is working correctly : 
                    dont overshoot the number of iterations and make sure sufficient amount are present , at a time there will occur saturation, 
                    when it does, keep it nearly that value.
                    values of a to try -> should be varied on the data 

            2.Feature Engineering : 
                to calculate a new variable or a feature using the given features e.g: area = length x breadth 

            3.Polynomial Regression: 
                we can change the data by adding a feature to keep the trend e.g: w1x + w2x^2 -> w1x+ w2x^2 + w3x^3
                * must to use feature scaling when we use Polynomial regression to convert it to a nearly linear regression .
                

    Classification: 
        binary classification..

        1.basic simple questions : we use [binary classification].
            is email a spam - yes or no -> classification binary since we use yes or no 
            class = category 

        2.Logistic Regression: 
            we use sigmoid function in order to seperate the classes into binary 
            threshold - we use a proper best fit line to match the dataset , 
            it outputs between 0 and 1 
                sigmoid  = 1 / (1+ e^-z) where 0<g(z)<1

            f(x) = g(z) = g(w.x + b)
            a.k.a - logistic regression 

            not all the time we get either 1 or 0 , sometimes we get a decimal between like: 0.2, 0.4 etc
            this indicates the probablity that the output will be 1.
            p(y=0)+ p(y=1) = 1

            f(x) = P(y=1|x;w,b) -> probablity that y=1 given x , parameters are w and b

        3.Decision Boundary:
            if we have a value below a Boundary we seperate it into a lower class and above it to a higer class.

            wkt g(z) = g(w1x1+w2x2+b)
            decision boundary z = 0 
                x1+x2 = 3 when b=3 
                therefore if a data has < 3 -> class 1 
                          else class 2
                
                *this applies to all shapes, a line , circle etc.
                Non - linear decision boundaries -> may be a more complex shape or line 
        
        4.Cost Function for Logistic Regression 
            it remains the same almost entirely apart from the part where the F(x) changes to sigmoid function...
            -> i.e F(x) = 1/(1+e^(-w.x-b))
            for n features and m datasets -> we would be having w1 + w2 + w3 ... n+b parameters

            squared error cost :
                what happens if there are multiple minimas locally 
                we need to come up with a new strategy >>>

            logistic loss fuction:
                we use a logarathimic graph to represent when we have 0 or 1
                i.e if f(x)->1 then loss ->0
                and if f(x)->0 then loss -> inf
            
                for y = 1 => -log(f(x))
                for y = 0 => -log(1-f(x))

        5.Simplified Cost Function for Logistic Regression
            we have L(f(x),y) = -y*log(f(x))-(1-y)log(1-f(x))
                this function automatically eliminates the other possibility of y 
            if we divide it by m then it will be a cost function : 
                j(w,b) = 1/m(-y*log(f(x))-(1-y)log(1-f(x)))

        6.problem of overfitting :
            we fit the graph too much , i.e passes through the exact points 
            1.underfit -> it has high bias and less variance[a lot of error], cold af.
            2.properfit -> generalized data where it just fits exactly enough [minimum error acceptable], just the right temperature .
            3.overfitting -> fits the training set extremely well, it has a lot of variance, and is tightly packed, too hot .

        7.addressing overfitting:
            if we dont have sufficient data then we go through a crisis of underfitting therefore we need right amount of features .

            reducing overfitting: 
            1.Collect more data:
                increasing the number of training data.
            2.Select Features
                reduce the number of features that need to be trained: not all features are important 
            3.regularization[reducing the size of parameters a.k.a weights]: 
                reducing the features results in loss of information 
                keep all the features , but reduce the weights of each features and parameters.
                works well when we have lots of features 

        8.Cost Function with regularization: 
            L2 regularization ->
            we introduce a new parameter called lambda -> regularization parameter that selects the important feature 
            therefore the regularization term will be [(lambda/2*m) * summ[wj]] 
            * there is a seperate term for b, but since its insignificant we shall ignore it for our convinence

            our main goal is to minimize cost function , therefore lambda balances -> -fit data and -keeping wj small 
                - if lambda is very very large then every features are ignored and tend to 0 hence we are just left with b. causes more regularization.
                - therefore we have to choose lambda in such a way as to only choose proper features 
                - a large amount of regularization parameter causes underfitting    
        
        9.Regularized linear Regression:
            we optimise wj' =  in Gradient descent by using the normal gradient descent + [lambda/m * wj]
            we can leave b' as it is cause there will be no difference

            once we simplify the equation: 
                shrink wj = (1-a*lambda/m) -> causes all w's to shrink and the lesser one's get's neglected.

        10.Regularized Logistic Regression: 
            we find the regular cost function which is similar to the one in linear regression 
            once we step into gradient descent : 
                we have to include lambda in here as well, regularization parameter is hence useful to decrease the unwanted parameters 
        
        EOF of supervised learning ...






Advanced Learning Algorithims: 
    Neural Networks: 
        algorithims that try to mimic the brain .
        a group of neurons form the neural layer that takes in multiple outputs and gives out multiple outputs .
        mulitple hidden layers are layers in between that process the data from input to output.

        eg: in image recognition-> 1.st layer - processess the edge and gives it to further layers.
                                   2.nd layer - processess the features like nose, ear, eyes etc. using the edges 
                                   3.rd layer - recognises the face using the features and recognises the perfect image.
        
        feed in the data -> input layer. 
        then pass in to the first layer .
        then we process it at multiple layers .
        we get a output after all the layers have done their work. 

    Advanced Neural layer
        1.we layer the data based on superscript -> denotes which layer has processed them and if its input -> a then its based on which layer has provied it .

        activation function -> aj[l] = g( wj[l] * a[l-1] + bj[l] )  //parameters w and b of layer l and unit j. while input a is of previous layer .

        code -> for neural network: 
        x = np.array([[200.0,17.0]]) --> input for the first layer.
        layer_1 = Dense(units=3,activation ='sigmoid' ) --> the neural layer 1 containing 3 neurons and sigmoid as activation function .
        a1 = layer_1(x)
        layer_2 = Dense(units =1, activation = 'sigmoid)
        a2 = layer_2(a1)
        output -> a2 

        once we get a2 we use threshold value to categorise the input as yes or no. 
    
    Data in Tensorflow:
        its a python library for deep learning :
            for a 2x3 matrix = np.array([[1,2,3],
                                         [4,5,6]])

            tf.Tensor([[0.8]],shape = (1,1), dtype = float32)//storing data into tensor instead of numpy 
            a2.numpy() -> activation function 
            array([[0.8]], dtype=float32) -> numpy always gives out 1-d array , and the results are always 1-d 
            meanwhile tensorflow provides a multiple dimensionality resutlt.

    building a Neural Network:
        Dense function ?
        def Dense(units,activation = None) -> it takes in the number of neurons and activation function as input and gives out the output of the layer .
    

Train a Neural Network in TensorFlow: 
    1. define the model using sequential model -> manual hyperparameters
        model = sequential([
            Dense(units=25, activation ='sigmoid')
            Dense(units=15, activation ='sigmoid')
            Dense(units=1, activation ='sigmoid')
        ])
    2. compile the model -> loss function and optimizer - module.compile
    3. train the model -> module.fit 

    -> loss and cost functions are the same thing :
        model.compile(loss = BinaryCrossentropy(), optimizer = SGD(learning_rate=0.2), metrics = ['accuracy']) -> gives out the loss function and optimizer -> logistic regression
        model.compile(loss = MeanSquaredError(), optimizer = SGD(learning_rate=0.2), metrics = ['accuracy']) -> linear regression
    -> Gradient descent ->
            model.fit(x_train,y_train, epochs = 100, batch_size = 100, verbose = 2) -> x_train and y_train are the training data
        and epochs are the number of times the data is passed through the model.


Activation Functions: 
    uses g(z) as activation function , 

    the most important activation function -> 
        RELU -> rectified linear unit -> g(z) = max(0,z) -> if z is negative then g(z) = 0 else g(z) = z
        Regression -> linear activation function -> g(z) = z
        Binary Classification -> sigmoid activation function -> g(z) = 1/(1+e^-z)

    how to choose activation function: 
        1. what do we use for output layer?
            -if its a binary classification problem we use sigmoid activation function.
            -if its a regression problem we use linear activation function.
            -if we are using for positive output we use RELU activation function.

        2. what do we use for hidden layers?
            -we use RELU activation function for hidden layers.
            -if we have a lot of layers we use RELU activation function for all the layers except the last one.
            -if we have a lot of layers we use sigmoid activation function for the last layer.

        Q. what do we use if its a binary classification on both hidden and output layers?
            -we use sigmoid activation function for both hidden and output layers.
        
        Q. what do we use if its a regression problem on both hidden and output layers?
            -we use linear activation function for both hidden and output layers.
        
        Q. what do we use if its a positive output problem on both hidden and output layers?
            -we use RELU activation function for both hidden and output layers.
    
    why do we need activation function ?
        -if we dont use activation function then the output will be a linear function of the input.
        -if we use linear activation function for all the layers then the output will be a linear function of the input.
        -if we use sigmoid activation function for all the layers then the output will be a sigmoid function of the input.
        -if we use RELU activation function for all the layers then the output will be a RELU function of the input.
        -if we use sigmoid activation function for all the layers except the last one then the output will be a sigmoid function of the input.
        -if we use RELU activation function for all the layers except the last one then the output will be a RELU function of the input.
        -if we use sigmoid activation function for the last layer then the output will be a sigmoid function of the input.
        -if we use RELU activation function for the last layer then the output will be a RELU function of the input.


    Multiclass classification example: 
        softmax activation function -> g(z) = e^zj/sum(e^zk) -> gives out the probability of the output being the class j.
        therefore in order to classify 10 objects we need 10 neurons in the output layer.
        the neuron with the highest probability is the output of the model.

    --> Logistic regression is a Genralized form of Softmax function .

        softmax function -> g(z) = e^zj/sum(e^zk) 
        loss(a1. . .  aN,y) = -loga1 if y = 1
                            = -log(1-a1) if y = 0 => -log(a2) if y = 0
                            it goes on ...
        
        model.compile(loss = SparseCatrgoricalCrossEntropy())


        model.compile(loss = BinaryCrossentropy(from_logits = True)) -> to be more numerically accurate and avoid Numerical roundoff error.
            

MultiLabel Classification: 
    it labels the data as present or not depending on the dataset , more than classification 

Multi-label : 
    adams algorithim [adaptive movement estimation] = adam(learning_rate = 0.01) -> adam is a optimizer algorithim.
    it varies the learning rate [alpha] based on the gradient descent.




->> Debugging a Learning algorithim:
    try getting more example 
    increasing or decreasing the learning rate[a] and changing the regularization value [lambda]

    Machine Learning diagnostic

    Evaluatig your model 
        since we'll be training our model on hundreds of parameters we cannot visualise it , hence 
        we divide the data for training and testing.In doing so we can check how accurate our model is.
    
    we take a graph that is overfitting then we train it to minimize the cost function of the training dataset 
    then we test it using the testing dataset

    the difference between cost function and errror is there is no lambda [regularization] in error function.
    eg error -> the amount of error / the amount of dataset = 1/500 = 0.2% error[1 error in 500 dataset]

    Model selection : 
        there are 2 types of errors -> jtest = is the fraction of the test set that has been misclassified
                                       and jtrain = is the fraction of the train set that has been misclassified .

            -usually jtest is high and jtrain is low.
            -we check if jtest and jtrain are better?jtest is better estimate of how the model will generalise to new data than jtrain.
            
            -we can use jtest to select the model that we want to use.we select the model with the least amount of error.
            fit parameters by minimizing the jtest and jtrain .


            Bias - Jtrain 
            Variance - Jtest

            jcv = cross validation  = Jtest -> it is the error of the cross validation set.
            
            1. underfit - bias is high
                          jtrain is high 
                          variance is low
                          jtest is high  
            2. properfit - bias is low
                          jcv is low 
                          variance is low
                          jtest is low
            3. overfit - bias is low
                          jtrain is low
                          variance is high
                          jcv is high
    Learning curves : 
        jtest vs jtrain are both inversely proportional to each other.
        jtest vs m is inversely proportional to each other.[more the training data less the testing error]
        jtrain vs m is directly proportional to each other.[more the training data more the training error]

    therefore 
        -IF THERE IS MORE BIAS THEN INCREASING THE NUMBER OF TRAINING DATA WON'T HELP.
        -IF THERE IS MORE VARIANCE THEN INCREASING THE NUMBER OF TRAINING DATA WILL HELP.
    
        WE CAN DECREASE THE BIAS BY ADDING MORE FEATURES.
        BY INCREASING THE LAMBDA WE CAN DECREASE THE VARIANCE AND BY DECREASING IT WE ADD MORE BIAS INTO THE MODEL.
        


    BIAS AND VARIANCE IN NEURAL NETWORK: 
        does it do well in training set?
        no: high bias, try bigger network 
        yes: does it do well in test set/cross validation set?
            no: high variance, try regularization,try more data
            yes: done -> perfect model 


    Iterative loop of ML development : 
        1.choose architecture(model,data,etc)
        2.train and evaluate
        3.diagonistics(bias,variance and error analysis)

    
Decision Trees: 
    we aim on maximizing the purity of the leaf nodes.
    we use entropy to measure the purity of the leaf nodes.[entropy - measure of randomness]
    wehen do we stop splitting? : 
        * When a node is 100% one class
        • When splitting a node will result in the tree exceeding 
        a maximum depth 
        • When improvements in purity score are below a 
        threshold
        • When number of examples in a node is below a 
        threshold

    WE NEED TO HAVE HIGHER ENTROPY AT THE TOP AND LOWER ENTROPY AT THE BOTTOM.
    ENTROPY -> FORMULA : SUM(-p*log(p)) -> p is the probability of the class.
        information gain = entropy(parent) - entropy(children)
        therefore whichever has a higher entropy has the higher information gain. 
        and we take the highest information gain as our result.
    
    what happens when we have multiple features?
        we take the feature with the highest information gain and split it.
        we do this recursively until we get a pure leaf node.
    Order: 
        1. calculate entropy of the parent node.
        2. calculate entropy of the children nodes.
        3. calculate information gain.
        4. split the node with the highest information gain.
        5. repeat the process until we get a pure leaf node.
    
    what happens when we have multiple classes?
        we use one hot encoding. 
        if we have a continuous variable then we use threshold function to split the data.


    Regression Trees: 
        we use mean squared error to measure the purity of the leaf nodes
        we use mean absolute error to measure the purity of the leaf nodes
        

    the modules for decision trees: 
    -XGBoost  , XBGclassifier 
    its a opensource model for decision trees.
    fast efficient implementation of gradient boosting.


-> classification: 

    from xgboost import XGBClassifier
    model = XGBClassifier()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

-> Regression: 

    from xgboost import XGBRegressor
    model = XGBRegressor()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)




unsupervised_learning , recommender systems and reinforcement Learning:


UN-SupervisedLearning: 
    we don't have any labels in the dataset. 
    we try to find patterns in the dataset.
    we try to find clusters in the dataset.


    k-MEANS algorithim: 
        randomly initialize k cluster centroids.
        repeat until convergence:
            for every training example:
                find the nearest centroid and assign it to that cluster.
            for every cluster:
                compute the mean of the points assigned to that cluster and assign it to the centroid.

    Anomaly Detection: 
        Gaussian distribution
            mean and variance
            say x is a number then probability of x is given by p(x) = 1/sqrt(2*pi*sigma^2) * e^(-1/2*(x-mu)^2/sigma^2) where [mu is the mean and sigma is the variance.]
            
    Anomaly detection: 
        1.Choose n features xi that you think might be indicative of anomalous examples.
        2.Fit parameters mu1, ..., mun, sigma1, ..., sigman
        3.Given a new example x, compute p(x)
            if p(x) < epsilon, then flag this example as an anomaly. [epsilon is the threshold value]
            else this example is not an anomaly.

        therefore this is useful to check for anomalous data from the dataset.

    Anomaly vs SupervisedLearning

        Anomaly detection:
            -Very small number(0-20) of positive examples (y = 1)[exceptions]
            -Large number of negative examples (y = 0)
            -Many different "types" of anomalies. Hard for any algorithm to learn from positive examples what the anomalies look like;
             future anomalies may look nothing like any of the anomalous examples we've seen so far.
        Supervised learning:
            -Large number of positive and negative examples.
            -Enough positive examples for algorithm to get a sense of what positive examples are like, future positive examples likely to be similar to ones in training set.

    Choosing what features to use:
        - using as many features as possible 
        - create a new features which are relevant 