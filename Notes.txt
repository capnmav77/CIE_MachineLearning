supervised learning
    we'll be having labels and based on that we predict the outcome -> regression and classification .

1.regression house price prediction-> take in multiple values and then we map it on a graph
    based on this graph we predict the outcome of the data 
2. classification -> it's a either yes or no question 
    if there are more than 2 categories we use classification based on categories based on parameters
    as we add more parameters the model goes complex
    therefore usage of multiple parameters is always useful

un-supervised learning
    the data is'nt labelled therefore we don't know what should be done, therefore we won't be having any parameters -> categorization
    1.in unsupervised we group the data which are similar and classfy them as group 1 and etc.. 

    data only comes with input x and we dont have output labels y, hence algorithim has to find structure in the data

    -> clustering
    -> dimensionality reduction
    

introduction to pandas 
    type        pandas name 
    vector      .series
    (1-dim)
    matrix      .dataframe

linear regression model is all about minimizing the cost error by using linear regression function and finding the lowest part of the graph 


06062023
Linear Regression model part 1:                                                                   

    what is linear regression?
    -> finding the best fit line fit for your data
    
    how to find the best fit line? -> y=mx+c [m? , c?, we have a input x and it predicts output y]
        we use F(x) = wX + b : where w,b are parameters/coefficients/weights
        where x = 'input' - 'feature'
        where y = 'output' - 'target' 
        m = number of training examples

        for each values are refrenced by x^i and y^i where i is the position of the object in the dataset

        1. find the cost function : 

                the distance from the line to the actual point J(w,b) = 1/2m(sum|x^i - x^^i|) : where m is the number of table entries
            therefore cost = mean squared error .

            using the above formula we get the value of the J[w] and J[b].

            once we plot it in 2d we get a parabola.

            if we also want to include both the values i.e w and b then its a type of a hammock in 3d .

            in order to increase the accuracy of the model we need to minimize the cost of the model hence we need something called


        2. Gradient descent: 
            w = w- a*diff(J[w,b])/diff(w) // where a is the learning rate decided by us 
            similarly 
            b = b- a*diff(J[w,b])/diff(b)

            in order to get the minimum function we need to run them simultaneously as w depends on b 
            therefore we need to decide the learning rate as optimally as 

            how do we run them simultaneously?
            -> we store the w and b in temporary value and then update them simultaneously
                temp_w =  w- a*diff(J[w,b])/diff(w) 
                temp_b =  b- a*diff(J[w,b])/diff(b)

                w = temp_w
                b = temp_b
            
            if there are multiple minimas then we use squared error cost to find any one minima 

        3. multiple parameters: 
            it would be catastrophic to just include only one feature and predict the outcome based on that 
            therefore we use multiple factors like number of bedrooms , how new the house is etc. therefore 
            instead of using multiple factors into the equation we use vectors 
            i.e -> F(x) = w.X + b 
            where w.X = w1x1+ w2x2+ w3x3+ w4x4+ ... wnxn.
            
            w and x can be represented as a matrix and can be multiplied consecutively to get the dot product 
            we can also use f = np.dot(w,x) //vectorised calculation
            therefore we get w and x for each home respetively 
                i.e w1,w2,w3,w4 etc. 
                we can either run a gradient descent on a single value or better consider them as a vector and run gradient descent on vectorised values 


        3.2 alternative for gradient descent 
            -> NOrmal Equation:
                only for linear regression with one variable 
                doesnt generalise to other learning algotithims 
                Normal method may be used in libraries but gradient descent is used widely cause it accounts in mutliple
                features and vector calculation can be done easily as well
